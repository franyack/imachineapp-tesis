%---------------------------------------------------------------------
%
%                          Capítulo 4
%
%---------------------------------------------------------------------

\chapter{Descripción del sistema}


\begin{FraseCelebre}
	\begin{Frase}
		La manera de empezar es dejar de hablar y empezar a hacer.
	\end{Frase}
	\begin{Fuente}
		Walt Disney
	\end{Fuente}
\end{FraseCelebre}

\begin{resumen}
En este capítulo se describe en primer lugar el diseño elegido para el sistema, explicando cada uno de sus componentes y justificando su composición. Luego, se explican todas las cuestiones técnicas relacionadas al motor de procesamiento, como por ejemplo la manera en que se caracterizan las imágenes, como se mide la similitud entre ellas, como se aplica el algoritmo MCL para obtener los grupos finales, etc. Por último, se presentan las herramientas con las que cuenta el usuario tanto para su experiencia durante la utilización de la aplicación como para administrar el resultado final.
\end{resumen}

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/imachineapp-logo}
		}
		\caption{Ícono de la aplicación desarrollada.}
		\label{fig:imachinapp-logo}
	\end{center}
\end{figure}

El producto desarrollado en este proyecto se identifica como \emph{IMachineApp}, donde la I al inicio de su nombre simboliza las imágenes a procesar, la palabra \textit{Machine} hace referencia a la ``Máquina'' por la cual pasan las mismas para poder obtener el resultado. Por último, la sigla App deja en claro que el producto está destinado a ser desplegado en dispositivos móviles (ver Figura \ref{fig:imachinapp-logo}).





%-------------------------------------------------------------------
\section{Diseño del sistema}
%-------------------------------------------------------------------
\label{cap4:sec:estructura}

En esta tesis se desarrolló un software que se conoce como aplicación móvil. La misma fue diseñada para ser ejecutada en teléfonos inteligentes y \textit{tablets}, y permite al usuario elegir un directorio (o un conjunto de ellos) con imágenes y luego de un procesamiento por el que pasan las mismas, sugerirle una estructura organizacional, basándose en el rango de similitud o contexto que tengan entre ellas. A su vez, cuando finaliza el proceso, tiene la posibilidad de administrar el resultado, según su criterio para obtener así el resultado esperado, o bien deshacer todos los cambios y volver las imágenes hacia su estado original.

La aplicación, fue diseñada para que todo el proceso ocurra en 4 etapas:

\begin{enumerate}
	\item \underline{Elección del/los directorio/s a procesar:} consiste en elegir algún directorio del dispositivo que cuente con imágenes a procesar, como por ejemplo donde quedan alojadas aquellas fotos que fueron tomadas por las cámaras del dispositivo.
	
	\item \underline{Procesamiento de las imágenes a través del motor:} esta etapa cuenta con una serie de pasos que se encarga de extraer las características de las imágenes, procesar las mismas afín de medir su similitud y poder obtener un conjunto de grupos donde en cada uno de ellos estarán las imágenes que más relación tengan entre sí. Esta  etapa constituye el \textit{core} de la aplicación.
	
	\item \underline{Administración del resultado arrojado por el procesamiento:} una vez que el motor terminó con el proceso de las imágenes y encontró una estructura de organización sugerida, en esta etapa el usuario podrá realizar tareas típicas como eliminar, mover o copiar imágenes entre directorios, o bien eliminar un directorio completo, moverlo hacia otra ubicación, etc.
	
	\item \underline{Administración del resultado final:} luego de que la etapa de administracón de las carpetas sugeridas por el proceso ha finalizado, el usuario tendrá la posibilidad de tomar diferentes decisiones respecto al resultado obtenido: descartar todos los cambios y volver las imágenes a su versión original, moverlas hacia una nueva carpeta creada por el programa, o copiarlas y mantenerla en dos lugares diferentes.
\end{enumerate}

\subsection{Metodología de trabajo}

Para el cumplimiento óptimo de todos los objetivos del proyecto, se implementó el ciclo de vida del software siguiendo un modelo iterativo denominado Desarrollo Rápido de Aplicaciones (\textbf{RAD} del inglés, \textit{Rapid Application Development}). Fue creado en 1980 por James Martin, siguiendo el modelo tradicional de desarrollo de software en cascada \cite{beynon1999rapid}. El modelo está compuesto por cuatro etapas, donde en la primera de ellas se realiza la definición de requerimientos, siguiendo por el diseño del sistema, el desarrollo del mismo y por último la etapa donde el producto está listo para ser enviado a producción. Si bien tiene un grado de similitud al modelo en cascada, en las etapas de diseño e implementación se trabaja de forma iterativa e incremental hasta cumplir con los criterios de aceptación dispuestos (ver Figura \ref{fig:rad-model}).

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/rad-model}
		}
		\caption{Modelo RAD (IMAGEN A CAMBIAR)}
		\label{fig:rad-model}
	\end{center}
\end{figure}

Se consideró a RAD el tipo de metodología que mejor se adaptaba al proyecto, en base a las diferentes etapas que lo conformaban y a la manera en que se trabajaría en cada una de ellas. Esta decisión se debe a que el proyecto tuvo una componente fundamental que fue la experiencia de usuario, y como se ha podido observar en publicaciones académicas que comparan diferentes metodologías para llevar adelante proyectos de software, se considera que aquellas de desarrollo lineal (como el modelo en cascada, por ejemplo) no son apropiadas \cite{burns1985selecting}. Esto se debe a que los requerimientos de diseño se cumplieron de forma iterativa e incremental y no estrictamente en una única fase del proyecto, a fin de verdaderamente hacer lo que se solicitaba. En base a los estudios señalados anteriormente, los componentes relacionados a la experiencia de usuario se fueron incorporando y mejorando de forma iterativa gracias a la retroalimentación o \textit{feedback} que se obtuvo por parte de los \textit{stakeholders}. 
 
Una vez definida la ERS (Especificación de los requerimientos del software), el producto se realizó en cuatro versiones de manera incremental, donde en cada una de ellas se entregaron avances, sujetos a lo especificado en el documento predefinido. Al término de cada versión, se obtuvo un prototipo del producto, para luego poder refinar y evaluar qué cosas eran necesarias cambiar, agregar o quitar, con el objetivo de que se cumplan exitosamente los criterios de aceptación de los entregables que conformaban el proyecto. Cada versión estaba compuesta por una parte de diseño, implementación y testeo, y son denominadas por la metodología como \textit{time boxing} \cite{beynon1999rapid}. La finalización de todas las versiones conformó el producto buscado.

A continuación, se explicitan cada uno de los prototipos que conformaron la realización del proyecto, especificando lo que se realizó en cada una de ellos. Cabe aclarar una vez más que el producto cuenta con dos bloques principales a diseñar e implementar. El primero de ellos es la parte encargada de la extracción de características de las imágenes, procesamiento de las mismas y agrupamiento, y el otro bloque está conformado por la aplicación, encargada de realizar la gestión y administración del resultado obtenido luego del procesamiento.

\begin{itemize}
	\item \textbf{Prototipo 1:} Integración inicial de tecnologías y componentes de desarrollo.
	\begin{itemize}
		\item \underline{Sistema Android:} La tarea que debió cumplir fue mostrar al usuario un conjunto de directorios, pero siempre eligiendo uno por defecto. Este directorio ingresaba al motor de procesamiento. Después de que se ejecutaba el motor, debía mostrar por pantalla que el proceso había sido exitoso.
		
		\item \underline{Motor de procesamiento:} debía levantar la biblioteca TensorFlow y leer las imágenes ingresadas por la aplicación, devolviendo una simulación de grupos resultado para el conjunto de imágenes procesadas.
	\end{itemize}
	
	\item \textbf{Prototipo 2:} Agregación de nuevas características al sistema e implementación de la metodología de clustering en el motor.
	\begin{itemize}
		\item \underline{Sistema Android:} se agregó a lo ya realizado la opción de elegir un directorio determinado. Luego del proceso, debía crear directorios en el dispositivo y copiar cada imagen al directorio correspondiente.
		
		\item \underline{Motor de procesamiento:} se aplicó las metodologías definidas para realizar el procesamiento, devolviendo al sistema a qué grupo debía ir cada una de las imágenes.
	\end{itemize}
	
	\item \textbf{Prototipo 3:} Continuación del desarrollo de nuevas funcionalidades y presentación formal del motor de procesamiento.
	\begin{itemize}
		\item \underline{Sistema Android:} debía permitir la utilización de las diferentes tareas de administración una vez que el procesamiento fue realizado. Además, se agregó la opción de guardar el resultado final o eliminarlo y volver todo al estado original.
		
		\item \underline{Motor de procesamiento:} sufrió modificaciones a fin de lograr mejores resultados, además se trabajó en tareas de optimización.
	\end{itemize}


	\item \textbf{Prototipo 4:} Finalización del desarrollo e integración final de todos los componentes.
	\begin{itemize}
		\item \underline{Sistema Android:} debía presentar todas las interfaces de usuario terminadas y listas para utilizar, el ícono que distingue a la aplicación y la aprobación de todas las pruebas de validación.
		
		\item \underline{Motor de procesamiento:} sufrió modificaciones a fin de lograr mejores resultados. Debió superar las pruebas presentadas en el protocolo de experimentación.
	\end{itemize}
\end{itemize}

\subsection{Patrón de programación seleccionado}

Como se dijo en la Sección \ref{cap2:sec:dispositivos-moviles}, la aplicación será desarrollada para el sistema operativo Android, de manera nativa, afín de aprovechar todas las ventajas que las librerías proveen para los desarrolladores. El framework Android, no recomienda ninguna forma específica de diseñar aplicaciones. Eso, de alguna manera, hace a los desarrolladores más poderosos y vulnerables, al mismo tiempo \cite{mvp2017guide}.

Las aplicaciones pasan por muchos ciclos de vida donde se van agregando/extrayendo características, ocasionando estragos si la misma no se diseña adecuadamente con una ``separación de preocupaciones''. No es una buena práctica cargar de lógica a las actividades que se encargan de interactuar con el usuario, sino mas bien deberían ser clases por las cuales simplemente la información que interactúa entre el sistema y el usuario fluye, desligándose de cualquier tipo de lógica de negocio. Es por ello, que se decidió seguir el desarrollo de la aplicación bajo un patrón de diseño Modelo-Vista-Presentador (\textbf{MVP}).

\subsubsection{Modelo-Vista-Presentador}

Este patrón de diseño es un conjunto de pautas que, si se siguen de manera correcta, desacoplan el código para su reutilización y testeabilidad. Básicamente, divide los componentes de la aplicación basados en su función, denominado \textit{separación de preocupaciones}, es decir, MVP divide a la aplicación en tres componentes básicos \cite{mvp2017guide}:

\begin{enumerate}
	\item \textbf{Modelo:} es el responsable de manejar todo lo que corresponda a la lógica de negocio de la aplicación, procesando la información y tomando decisiones.
	
	\item \textbf{Vista:} es la responsable presentar las vistas al usuario e interactuar con el mismo.
	
	\item \textbf{Presentador:} es un puente que conecta al modelo con la vista. Además, actúa como un instructor para la vista.
\end{enumerate}

Definidos cada uno de los componentes, MVP establece algunas reglas básicas para ellos:

\begin{itemize}
	\item La única responsabilidad de una Vista, es dibujar la interfaz de usuario según las instrucciones del Presentador. Es la parte ``tonta'' de la aplicación.
	\item La Vista delega todas las interacciones con el usuario hacia el Presentador.
	\item La Vista \textbf{nunca} se comunica con el Modelo directamente.
	\item El Presentador es el responsable de delegar los requerimientos de la vista hacia el Modelo, y instruir a la Vista con acciones para eventos específicos.
	\item El Modelo es el responsable de realizar toda la lógica de negocio requerida, como por ejemplo obtener los datos desde el servidor, la base de datos o el sistema de archivos.
\end{itemize}

A modo de resumen, cada Vista tendrá una relación 1 a 1 con un Presentador, el cual se encargará de llevar y traer toda la información, indicándole a la Vista que acción y en qué momento se debe ejecutar. Dicha comunicación se establece a través de Interfaces, las cuales servirán como \textit{contratos} para dejar pre-establecidos que medios de comunicación existirán entre ellos. De manera similar y bajo el mismo criterio, el Presentador se comunicará con el Modelo a través de Interfaces, pero sin la necesidad de que la relación entre ellos sea estrictamente 1 a 1, ya que el modelo podría modularizarse tanto como se requiera. De esta manera, el Modelo se desentiende de los eventos y sólo es llamado cuando se lo requiere (ver Figura \ref{fig:mvp-pattern}).

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/mvp-pattern}
		}
		\caption{Patrón de diseño Modelo-Vista-Controlador}
		\label{fig:mvp-pattern}
	\end{center}
\end{figure}

La aplicación fue pensada en dos bloques diferentes. El primero se corresponde a la parte principal, donde se elige el directorio a procesar y se lleva adelante toda la lógica para lograr la sugerencia de organización de las imágenes. El segundo, ya tiene más que ver con las opciones de administración sobre el resultado, con la posibilidad de guardarlo, de mover todas las imágenes hacia un nuevo directorio final o bien descartar todos los cambios. En la Figura \ref{fig:IMachineAppMVP} se puede ver en lineas generales el diseño de la aplicación.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/IMachineAppMVP}
		}
		\caption{Diseño de la aplicación IMachineApp}
		\label{fig:IMachineAppMVP}
	\end{center}
\end{figure}

Como conclusión y aprendizaje, la arquitectura es en lo primero que se debe trabajar para cualquier software. Una arquitectura cuidadosamente diseñada, minimizará muchas repeticiones en el futuro, a la vez que proporcionará la facilidad de escalabilidad. La mayor parte de proyectos se desarrollan hoy en día en equipo, por lo que la legibilidad y la modularidad del código deben considerarse como elementos sumamente importantes de la arquitectura. También, se depende en gran medida de bibliotecas de terceros, cambiando entre alternativas, debido a casos de uso, errores o soporte. 

\section{Motor de procesamiento de las imágenes}

En esta sección se detallará el paso a paso por el que transita el conjunto de imágenes para que se puedan obtener los grupos sugeridos por la aplicación, ordenándolas según el grado de similitud o contexto que tengan entre ellas.

El motor de procesamiento de las imágenes se llama \textbf{CIEngine}. El nombre está compuesto por las letras C, haciendo referencia a la palabra \textit{Clustering} (agrupamiento en inglés), la letra I de \textit{Images}, y por último la palabra \textit{Engine}, que en inglés significa motor. El mismo fue pensado y realizado como un módulo aparte de la aplicación propiamente dicha, y agregado a través de un paquete. Esto tiene como beneficios que sea independiente, por lo que la mantenibilidad y testeo se realizan sin depender exclusivamente de la aplicación, además de que se puede pensar como trabajo a futuro en la adaptación a otros ambientes, como puede ser en un programa escritorio, un servicio alojado en la nube, una aplicación nativa para otros sistemas operativos utilizados en dispositivos móviles, etc.

En la Figura \ref{fig:CIEngineStep-by-step} se puede observar el camino que recorren las imágenes dentro del motor, desde la lectura de todas ellas hasta la conformación de cada uno de los grupos resultantes. Las tareas fueron divididas en etapas, afín de que el trabajo esté estructurado y sea más entendible, localizando en cada una de ellas diferentes técnicas, relacionadas entre sí. A continuación, se describe en detalle cada uno de los pasos.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/CIEngineStep-by-step}
		}
		\caption{Diseño del motor de procesamiento}
		\label{fig:CIEngineStep-by-step}
	\end{center}
\end{figure}

\subsection{Etapa 1: Pre-procesamiento y Extracción de Características}

Como se dijo en el capítulo anterior, se utiliza la biblioteca TensorFlow, específicamente un modelo de red neuronal denominado MobileNet, para realizar la descripción y extracción de características de las imágenes. Se eligió este tipo de modelo ya que se encuentran preparados y optimizados para ser utilizados en dispositivos móviles, logrando buenos resultados en ambientes productivos ya que son hoy por hoy el estado del arte en lo que corresponde a problemas de clasificación de imágenes \cite{introCNN}. 

La red utilizada en este proyecto fue entrenada con 1001 grupos pertenecientes a la base de datos de imágenes \textit{ImageNet}. Esta base pertenece a un proyecto que fue diseñado para el uso de sistemas relacionados al reconocimiento de objetos, ya que cuenta con más de 14 millones de URL's de imágenes que han sido anotadas a mano por personas para indicar qué son los objetos que se encuentran en cada una de las imágenes. La base de datos cuenta con más de 20.000 categorías, y si bien las anotaciones de las imágenes realizadas por los colaboradores está disponible de manera gratuita, las imágenes reales no son propiedad de \textit{ImageNet} \cite{ImageNet}.

La forma de procesar que tiene la red es: a partir de una imagen de entrada, devolver el grado de similitud que tiene con los 1001 grupos con los que cuenta la red entrenada. Esto significa que por cada uno de los grupos, se obtendrá como salida un número entre 0 y 1, indicando el porcentaje de similitud. La suma de los 1001 resultados, será 1 (ver Figura \ref{fig:MobileNet}.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/MobileNet}
		}
		\caption{Ejemplo de aplicación de una MobileNet}
		\label{fig:MobileNet}
	\end{center}
\end{figure}

A su vez, estos grupos están organizados acordes a la jerarquía de etiquetas o \textit{tags} que tiene \textit{WordNet}, una gran base de datos léxica para el idioma inglés, compuesta de sustantivos, verbos, adjetivos y adverbios que se agrupan en conjuntos, donde cada uno de ellos expresan un concepto distinto. Para el uso de \textit{ImageNet}, sólo se utilizaron \textit{tags} sustantivos, y los mismos están jerarquizados partiendo desde uno madre o genérico, bajando hacia aquellos que detallan en concreto el/los objetos que se encuentran en la imagen (ver Figura \ref{fig:WordNetExample}). Esta base también es libre y gratuita, enfocada principalmente en el procesamiento automático del lenguaje natural y en aplicaciones de inteligencia artificial \cite{WordNet}.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/WordNetExample}
		}
		\caption{Ejemplo del uso de etiquetas - WordNet}
		\label{fig:WordNetExample}
	\end{center}
\end{figure}

Dichas anotaciones, tienen relación con una base de datos de etiquetas o \textit{tags} que fue organizada acorde a la jerarquía de palabras que se encuentran en la base de datos léxica WordNet. Cada una de las palabras que se encuentran allí están internacionalizadas por medio de relaciones conceptuales-semánticas y léxicas. Como sucede con ImageNet, esta base de datos también es libre y pública por lo que se encuentra disponible para su descarga.


 
logrando un tamaño total de 16.9MB, ideal para el uso en una aplicación móvil, ya que se busca que la misma no ocupe tanto espacio, sumando la no necesidad de una conexión a Internet, haciendo todo el proceso explotando los recursos del dispositivo.

 


El pre-procesamiento que se le realiza a las imágenes

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Arranca lo de LEA}

Tal como se aclaró en la Sección \ref{cap1:subsec:restricciones}, el código fuente de IMachineApp sigue un Diseño Orientado a Objetos (\acs{DOO}) el cual cuenta con los siguientes beneficios para desarrolladores \cite{fayad1997object}:
\begin{itemize}
	\item \emph{Modularidad}, al encapsular detalles de la implementación detrás de interfaces estables y sencillas de utilizar.
	\item \emph{Reutilización}, definiendo componentes genéricos que pueden ser reaplicados para crear nuevas aplicaciones. Con ello se aprovecha el dominio de conocimiento y el esfuerzo previo de desarrolladores experimentados para evitar rehacer y revalidar soluciones ya existentes.
	\item \emph{Extensibilidad}, al proveer métodos acoplables que permiten a las aplicaciones extender sus interfaces estables.
	\item \emph{Inversión de control}, al invertir el flujo de ejecución del programa dejando que alguna entidad lleve a cabo las acciones de control que se requieran, en el orden necesario y para todos los sucesos que deban ocurrir, en lugar de hacerlo imperativamente mediante llamadas a procedimientos o funciones.
\end{itemize}

% LINKS: http://dirkriehle.com/computer-science/research/dissertation/chapter-4.html; http://www.codeproject.com/Articles/5381/What-Is-A-Framework; http://paginas.fe.up.pt/~aaguiar/as/acm-out97-p32-fayad.pdf

La última propiedad mencionada refiere a que por medio del framework se explicita una solicitud concreta (e.g. entrenar una red neuronal sobre un conjunto de datos), y el mismo decide la secuencia de acciones necesarias para atenderla. En cuanto a las demás, el resto del presente capítulo muestra evidencia de su cumplimiento mediante las características que se van presentando.


Como la mayoría de los proyectos desarrollados en Python, Learninspy está compuesto por paquetes que agrupan módulos en común, y en cada uno de estos últimos se reúnen las clases (en términos de DOO) que tengan mayor relación. A continuación se describe brevemente la lógica de cada módulo y paquete, aunque para mayor detalle se debe consultar el manual de referencia \footnote{Documentación de Learninspy: \url{http://learninspy.readthedocs.io/}}:
\begin{itemize}
	\item \textbf{Core}: Como bien dice el nombre, es el módulo principal o el núcleo del framework. El mismo contiene clases relacionadas con la construcción de redes neuronales profundas, desde la configuración de los parámetros usados hasta la optimización del desempeño en las tareas asignadas. Se detallan entonces cada uno de los submódulos que lo componen:
	\begin{itemize}
		\item[+] \textit{Activations}: En el mismo se implementan las funciones de activación (con su correspondiente derivada analítica) que se podrán utilizar en las capas de una red neuronal. 
		\item[+] \textit{Autoencoder}: Se extienden las clases desarrolladas en el submódulo \textit{model}, mediante herencia de métodos y atributos, para implementar autocodificadores y su uso en forma apilada.  
		\item[+] \textit{Loss}: Provee dos funciones de error, las cuales son utilizadas en base a la tarea designada a una red neuronal: clasificación, mediante la función de \textit{Entropía Cruzada}, y regresión, con la función de \textit{Error Cuadrático Medio}.
		\item[+] \textit{Model}: Es el submódulo principal de \textbf{core} ya que contiene las clases referidas directamente a redes neuronales, el diseño de sus capas y la configuración de los parámetros que manejan.
		\item[+] \textit{Neurons}: Este submódulo contiene una clase para manejar las matrices de pesos sinápticos y los vectores de sesgo que componen las capas de una red neuronal. Dichos arreglos se implementan mediante NumPy para que se almacenen de forma local (i.e. se alojan por completo en un mismo nodo físico de ejecución), aunque se tiene pensado extender esta clase para que puedan manejarse en forma distribuida. 
		\item[+] \textit{Optimization}: Implementa los algoritmos y funcionalidades de optimización que se utilizan para mejorar iterativamente el modelado de las redes neuronales. Los algoritmos presentes han sido explicados en la Sección \ref{cap2:subsec:optimizacion} (salvo Adagrad, ya que en su lugar se implementó Adadelta) y para la implementación fueron adaptados desde el desarrollo hecho en Climin \cite{bayer2016climin}, el cual es un framework de optimización pensado para escenarios de aprendizaje maquinal.
		\item[+] \textit{Search}: Realizado para abarcar algoritmos de búsqueda que optimicen los parámetros de un modelo en particular. El único algoritmo desarrollado en esta versión es el de búsqueda aleatoria, que fue detallado anteriormente en la Sección \ref{cap2:subsec:ajuste-parametros}.
		\item[+] \textit{Stops}: Recopila distintos criterios de corte para frenar la optimización de las redes en base a una condición determinada. Al igual que el submódulo \textit{optimization}, está basado en el trabajo hecho en Climin. 
	\end{itemize}
	\item \textbf{Utils}: Este módulo abarca todas las utilidades desarrolladas para posibilitar tanto la construcción de redes neuronales como el funcionamiento total del framework. El mismo dispone de los siguientes submódulos:
	
	\begin{itemize}
		\item[+] \textit{Checks}: Contiene funcionalidades para comprobar la correcta implementación de las funciones de activación y de error, basándose en las instrucciones de un tutorial de aprendizaje profundo \cite{ng2012ufldl}.
		\item[+] \textit{Data}: Es el submódulo principal de \textbf{utils}, ya que posee clases útiles para construir los conjuntos de datos que alimentan las redes neuronales, y también funcionalidades para muestrearlos, etiquetarlos, partirlos y normalizarlos.
		\item[+] \textit{Evaluation}: Se proporcionan clases para evaluar el desempeño de las redes neuronales en tareas de clasificación y regresión, mediante diversas métricas que fueron explicadas en la Sección \ref{cap2:subsec:metricas}. 		
		\item[+] \textit{Feature}: Se implementan funcionalidades referidas a extracción de características o tipos de pre-procesamiento sobre los datos que alimentan una red neuronal. Un ejemplo de ello es el análisis de componentes principales o PCA (mencionado en la Sección \ref{cap2:subsec:no-supervisado}), que fue implementado siguiendo tutoriales clásicos de deep learning \cite{li2015cs231n} \cite{ng2012ufldl}.
		\item[+] \textit{Fileio}: Submódulo con funciones para realizar manejo de archivos y la configuración del logger de Learninspy.
		\item[+] \textit{Plots}: Reúne todas las funcionalidades referidas a gráficas y visualizaciones (como el ajuste de una red durante el entrenamiento).
	\end{itemize}
\end{itemize}

Además, a la misma altura que estos dos módulos, existe un script denominado \textit{context} en donde se configura e instancia el contexto de Spark a utilizar en el framework. En la Sección \ref{cap5:subsec:rendimiento-spark} del siguiente capítulo se mencionan las configuraciones que contempla este script referidas al rendimiento de Spark.

En base al diseño planteado, se identifican dos perfiles de acceso al framework: a) de usuario, en el cual mediante conocimientos básicos de Python se puede utilizar la plataforma y añadirle algunas funcionalidades, siguiendo un paradigma de programación imperativa; b) de desarrollador, que requiere usar un paradigma de programación orientado a objetos y funcional, para la comprensión total del código mediante conocimientos de Python y Spark.

La estructura presentada se considera exhaustiva en cuanto a contenido del framework, por lo cual cualquier desarrollador que quiera modificar o agregar componentes al mismo debería poder valerse de los módulos disponibles en la arquitectura comprendida.

%-------------------------------------------------------------------
\section{Características}
%-------------------------------------------------------------------
\label{cap4:sec:features}

A continuación, se detallan las particularidades de Learninspy que lo hacen un framework útil para construir redes neuronales con aprendizaje profundo sobre un conjunto de datos y en forma distribuida:
\begin{itemize}
	\item \textit{Diseño que permite extender funcionalidades con pocas modificaciones y sin romper el funcionamiento de otros módulos}. 
	
	Esto se relaciona con la propiedad de \textit{extensibilidad} en un framework, mencionada en la anterior Sección \ref{cap4:sec:estructura}. Por ejemplo, para agregar una función de activación y su derivada analítica, basta con incorporar sus definiciones en el submódulo \textbf{core.activations} y, mediante una etiqueta apropiada, adjuntarlas a los diccionarios de Python (que se encuentran al final del módulo) para utilizarlas en el framework a través del mismo. Se puede realizar un tratamiento similar para agregar tanto funciones de error como algoritmos de optimización y sus criterios de corte. 
	
	\item \textit{El paradigma orientado a objetos permite aprovechar la naturaleza del diseño de las redes neuronales, para así expresar las relaciones existentes entre las entidades manejadas}.
	
	Por ejemplo, la composición de una red neuronal por capas de neuronas, donde cada una de ellas tiene asociado una matriz de pesos sinápticos y un vector de sesgo, y también el hecho de que un autocodificador sea un tipo especial de red neuronal por lo que tiene una relación de herencia de métodos y atributos.
	
	%\item \textbf{Durante el diseño e implementación se priorizó la estructuración simple y lograr fácil legibilidad en el código fuente, tal como se ha definido en el alcance del proyecto.}
	
	%Es por ello que, a pesar de que Spark se encuentra codificado en el lenguaje Scala, se eligió implementar el sistema en Python utilizando la API de ese lenguaje que provee el motor usado. A raíz de ello, ¿se sacrifica en parte la eficiencia del motor que podría lograrse utilizando el lenguaje nativo Scala?, a costa de conseguir un framework fácil de usar y reutilizar en trabajos desde el código fuente. % \footnote{\url{https://datasciencevademecum.wordpress.com/2016/01/28/6-points-to-compare-python-and-scala-for-data-science-using-apache-spark/}}.
	
	\item \textit{Mínima cantidad de dependencias en el sistema}.
	
	A partir del énfasis que se tuvo en esta propiedad para el diseño, no se requiere instalar más que Spark (y Java por ello) y parte del ecosistema de SciPy (que es casi un estándar en las típicas aplicaciones de Python).
	
	\item \textit{Optimización de un modelo mediante entrenamiento de réplicas en forma concurrente y distribuida}.
	
	Es la característica principal de optimización que se diseñó para el sistema, y es explicada detalladamente en la siguiente Sección \ref{cap4:sec:paralelismo}.
	
	%\item \textbf{La función de consenso que hace la mezcla de los modelos entrenados en paralelo puede también implementarse}. 
	
	%Este grado de libertad otorga gran flexibilidad y potencia al framework, 
	
	%(ventaja respecto a H2O). (está en el módulo \textbf{core.optimization})

	\item \textit{Los resultados del modelado pueden reproducirse de forma determinística}

	A diferencia de otras herramientas que distribuyen las operaciones de modelado, en Learninspy es posible replicar de forma exacta un experimento con una configuración dada. Esto se debe a que internamente se gestiona en forma determinística el semillero que alimenta el generador de números aleatorios, los cuales son requeridos por varios algoritmos que intervienen en el modelado (e.g. inicializador de pesos sinápticos, DropOut, etc). 
	
	
	\item \textit{Soporte para procesar conjuntos de datos en forma local y distribuida}
	
	Mediante las funciones y clases del módulo \textit{utils.data} presentado, se brindan funcionalidades para el tratamiento de datos tanto en forma local como distribuida (utilizando RDDs de Spark para este último caso).
	
	\item \textit{Soporte para cargar y guardar modelos entrenados}. 
		
	El trabajo de optimización de los modelos se puede realizar de forma diferida, ya que los mismos se pueden guardar y volver a cargar en formato binario. Esto tiene gran utilidad sobre todo cuando se someten a aprendizaje no supervisado, el cual puede realizarse en muchas pasadas hasta aplicarse el ajuste fino.
	
	
	% \item  Quizás conviene poner un diagrama de bloques referido al pipeline de datos recomendado para su tratamiento en Learninspy.
\end{itemize}

Como se puede ver, algunas características están referidas al diseño del software en general y otras son más específicas del procesamiento distribuido que involucra. Por lo tanto, se describe a continuación en qué formas se logran integrar estas características en el framework.
	
	
%-------------------------------------------------------------------
\subsection{Explotación del cómputo distribuido}
%-------------------------------------------------------------------
\label{cap4:subsec:compdistrib}

Como ya se dijo anteriormente en otras secciones, las aplicaciones que suelen tratarse con aprendizaje profundo están relacionadas con datos de gran dimensión, y por ello las herramientas que realizan dicho tratamiento requieren una ventaja computacional para resultar útiles en ello. Las formas en que Learninspy aprovecha el procesamiento distribuido de Spark son las siguientes:
\begin{enumerate}
	\item \textbf{Preparar conjuntos de datos:}
	El framework provee una abstracción para manejar conjuntos de datos, la cual incluye el etiquetado de los patrones por clases, la normalización y escalado de los datos, el muestreo balanceado por clases, etc. Para grandes volúmenes de datos se provee una interfaz adecuada para los RDDs de Spark, con lo cual el pre-procesamiento puede realizarse en forma distribuida.
	
	\item \textbf{Optimizar modelos en forma paralelizada:}
	Siendo quizás el valor principal del procesamiento distribuido en el framework, esta característica se basa en que, por cada iteración del ajuste de una red neuronal, el modelado se realice mediante instancias replicadas que se entrenan de forma independiente y luego convergen en un modelo único, reuniendo así las actualizaciones que adquirió cada instancia por separado.
	
	\item \textbf{Ahorrar costos de comunicación, transfiriendo conjuntos de datos a los nodos por única vez (broadcasting):}
	Como se explicó en la Sección \ref{cap3:subsec:spark-funcionalidades}, la funcionalidad de Broadcast que provee Spark permite que una variable muy utilizada se pueda enviar a los nodos computacionales una sóla vez (siempre que la usen únicamente en modo lectura). Esto resulta útil y eficiente con los conjuntos de datos empleados en el ajuste de las redes neuronales, el cual se hace iterativamente y de otra forma requeriría establecer una comunicación con los nodos activos por cada iteración.
	
	\item \textbf{Configurar infraestructura fácilmente:}
	Mediante simples configuraciones en las variables de entorno, se puede conectar el framework forma sencilla a una estructura computacional definida con Spark (lo cual se menciona más adelante en la Sección \ref{cap5:sec:configuraciones}).
	
\end{enumerate}

Para entender cómo se obtiene la segunda característica mencionada, que se considera la más importante y tiene cierta complejidad, la siguiente sección detalla la forma en que se implementa en Learninspy.

%-------------------------------------------------------------------
\section{Entrenamiento distribuido}
%-------------------------------------------------------------------
\label{cap4:sec:paralelismo}

El procedimiento para minimizar la función de costo sobre una red neuronal es una característica clave de Learninspy, ya que es una de las formas principales en que se aprovecha el cómputo distribuido en el framework. Dado que los algoritmos de optimización utilizados para realizar ello son iterativos, la paralelización propuesta busca incorporar los beneficios de la concurrencia para sacar mayor provecho al proceso en cada una de sus iteraciones.

La idea no es nueva ya que es implementada en diversos esquemas como los explicados en la Sección \ref{cap3:subsec:paralel-modelos}. Se basa en que el proceso de optimización de las redes neuronales se puede paralelizar de forma tal que se obtenga una mejoría en duración y hasta resultados respecto al procedimiento convencional sin concurrencia. Para ello se tiene que, por cada iteración del proceso, un modelo base es copiado a cada nodo computacional para que cada una de estas copias o réplicas se entrene de forma independiente sobre algún subconjunto muestreado del conjunto original de datos. El hecho de optimizar en cada iteración con un subconjunto de datos (conocidos como \textit{mini-batch}) en lugar del conjunto completo permite acelerar el proceso y está demostrado en varios estudios que aún así obtiene buenos resultados, como fue explicado en la Sección \ref{cap2:subsec:optimizacion}. Es preciso aclarar que dichos subconjuntos son obtenidos de un muestreo aleatorio sin reemplazos sobre el conjunto de entrenamiento, utilizando la función \textit{sample} de la librería \textit{random} que ofrece la versión usada de Python.

La cantidad de modelos replicados a entrenar en paralelo es configurable: para un mejor desempeño en términos de recursos, debe ser la cantidad de nodos/núcleos disponibles, pero también puede ser menor o mayor para tener otro impacto en los resultados. Una vez entrenadas las réplicas, se procede a mezclar los modelos de forma que converjan los aportes de la optimización en un único modelo. Para ello se emplea una ``función de consenso'' que toma los parámetros de cada modelo y los pondera en base al resultado de evaluación sobre los respectivos subconjuntos de datos que utilizaron.

En el Algoritmo \ref{alg:train} se esquematiza el procedimiento general que sigue el entrenamiento de una red neuronal en Learninspy. Notar que el mismo se estructura como una tarea MapReduce, ya que de esa forma es implementado mediante las primitivas de ese tipo que provee el motor Spark. Mediante la función \textit{merge} se realiza el proceso de mezclado de modelos mediante una función de consenso, lo cual se explica en detalle a continuación. 

	
\begin{algorithm}
	\caption{Entrenamiento distribuido en Learninspy}\label{alg:train}
	\begin{algorithmic}[1]
		\Require Modelo actual $h_{W,b}$.
		\Function{train}{$\Gamma$, $\mu$, $\rho$} \Comment{Parámetros: Conjunto de entrenamiento $\Gamma$; tamaño de mini-batch $\mu$; cantidad de modelos concurrentes o "paralelismo" $\rho$}
		\State --- MAP ---
		\State $H_{W,b} = copy\_model(h_{W,b}, \rho)$  \Comment{Realizar $\rho$ copias de $h_{W,b}$ sobre los nodos disponibles }
		\For {$H_{W,b}^{(i)} \forall i \in \{1, \dots, \rho \}$} \Comment{Bucle de ejecución concurrente}
		\State $\Gamma_\mu = sample(\Gamma, \mu)$ \Comment{Muestreo de $\mu$ ejemplos sobre el conjunto $\Gamma$} 
		\State $s_{i} = minimize(H_{W,b}^{(i)}, \Gamma_\mu ) $ \Comment{Optimización de modelo réplica}
		\EndFor
		\State --- REDUCE ---
		\State $h_{W,b} = merge(H_{W,b}, s)$  \Comment{Mezcla de modelos con función de consenso}
		\State $results = evaluate(h_{W,b}, \Gamma)$ \Comment{Evaluación sobre el conjunto de datos}
		\Return $h_{W,b}, results$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\iffalse
\begin{algorithm}
	\caption{Mezcla de modelos entrenados concurrentemente}\label{alg:merge}
	\begin{algorithmic}[1] 
		\Function{merge}{$H_{W,b}$, $f$} \Comment{Parámetros: Conjunto de modelos replicados a mezclar $h_{W,b}^{(i)} \forall i \in \{1, \dots, \rho\}$; función de consenso usada para mezclar modelos $f$.}
		\State $h_{W,b} = $
		\State --- MAP ---
		\For {$H_{W,b}^{(i)} \forall i \in \{1, \dots, \rho \}$}
			\For {$l \in \{1, \dots, L\}$}
				
			\EndFor
		\EndFor
		\State --- REDUCE ---
		
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\fi

\iffalse
\begin{python}
	class MyClass(Yourclass):
	def __init__(self, my, yours):
		bla = '5 1 2 3 4'
		print bla
\end{python}
\fi	
	
	

%-------------------------------------------------------------------
\subsection{Funciones de consenso}
%-------------------------------------------------------------------
\label{cap4:subsec:consenso}	

Una vez entrenados todos los modelos replicados de forma concurrente, se deben mezclar en uno solo tratando de reunir las contribuciones de cada uno al ajuste del modelo deseado. Para ello, se puede caracterizar a cada modelo optimizado por su desempeño o \textit{scoring} $s_i$ que es obtenido de dos formas posibles: por una métrica aplicada en su evaluación (e.g. \textit{accuracy} de clasificación, o $R^2$ de regresión), o bien por el valor resultante en la función de costo definida. El valor escogido para caracterizar cada modelo puede utilizarse como parte de una ponderación realizada sobre todos los modelos durante la mezcla, la cual consiste simplemente en una suma de los parámetros $W$ y $b$ de cada capa, por cada uno de los modelos correspondientemente. Para ello se propone usar una función de consenso que, en base a una ponderación establecida, logre reunir las contribuciones de los modelos para obtener un único modelo representativo. Esta mezcla consiste en una suma de los parámetros mencionados estableciendo pesos en base a una ponderación elegida, y esa suma a su vez es escalada por la sumatoria de los pesos obtenidos de la siguiente forma:  

\begin{equation}
 f(l_{W,b}, w) = \sum_i \frac{w_i l_i}{\sum w_i}
\end{equation}

Si el denominador es muy cercano a 0, el mismo se reemplaza por una constante $\epsilon = 1e-3$ para evitar divisiones por 0. 

Por defecto, se incluyen tres tipos de ponderación: a) constante, con los mismos pesos valiendo 1 para todos los modelos (resultando una media aritmética de cada parámetro), b) lineal, donde se utiliza en forma directa el valor de $s_i$, c) logarítmica, de forma que la ponderación no tenga gran variación sobre valores altos de $s_i$ (muy buen valor en la evaluación, o bien pésimo costo de la red):


\begin{subequations}
%\begin{align}
	
	\begin{align}
			w_i  & = 1, \qquad \forall i \in \{1, \dots, \rho\} \\
			w_i  & = s_i, \qquad \forall i \in \{1, \dots, \rho\} \\
			w_i  & = 1 + \ln(\max(s_i, \epsilon)), \qquad \forall i \in \{1, \dots, \rho\}, \qquad \epsilon=1e-3
	\end{align}
	
%\end{align}	
\end{subequations}

Notar que para la ponderación logarítmica, si el dominio es menor o muy cercano a 0 se reemplaza por una constante $\epsilon = 1e-3$ para evitar conflictos con el dominio de la función logaritmo. 

\begin{figure}
	\centering
	\subfloat[Sin ponderación (constante)]{{\includegraphics[width=0.38\textwidth]
			{Imagenes/Bitmap/Rplot-funcons-avg} }}%
	\qquad
	\subfloat[Ponderación lineal]{{\includegraphics[width=0.38\textwidth]
			{Imagenes/Bitmap/Rplot-funcons-w_avg} }}%
	\qquad
	\subfloat[Ponderación logarítmica]{{\includegraphics[width=0.38\textwidth]
			{Imagenes/Bitmap/Rplot-funcons-log_avg} }}%
	\caption{Función que describe los pesos $w$ que ponderan a cada modelo réplica en base a su valor $s$, suponiendo un dominio (0, 1] para dicho valor.}%
	\label{fig:consenso}
\end{figure}


En la Figura \ref{fig:consenso} se representan gráficamente las funciones mencionadas, para un dominio definido en los valores del \textit{scoring}. Para utilizar una función de consenso en particular, se debe configurar tanto la función como el \textit{scoring} que utiliza mediante los parámetros de optimización que se definen para el modelado. Para ello, se debe instanciar un objeto OptimizerParameters del módulo \textit{core.optimization} indicando dichos parámetros en sus argumentos (ver detalles de uso en el manual de referencia). 


\iffalse
\begin{python}
	def merge_models(results_rdd, criter='w_avg', goal='hits'):
	# Bloque de chequeos:
	# ...
	# Mezcla de modelos con la funcion de consenso definida
	layers = (results_rdd.map(merge_fun)
	.reduce(lambda left, right:
	mix_models(left, right)))
	total = results_rdd.map(weights).sum()
	# Ponderación sobre sobre todas las capas
	final_list_layers = map(lambda layer: layer / total, layers)
	return final_list_layers	
\end{python}
\fi

%-------------------------------------------------------------------
\subsection{Criterios de corte}
%-------------------------------------------------------------------
\label{cap4:subsec:criterios-corte}

En cualquier aplicación de aprendizaje maquinal, por lo general no se ejecuta la optimización de un modelo hasta obtener un desempeño deseado ya que puede ser que no se alcance dicho objetivo por la configuración establecida. Es por ello que, tal como se introdujo en la Sección \ref{cap2:subsec:control-optimiz}, resulta conveniente establecer ciertas heurísticas para monitorear la convergencia del modelo en su optimización. Un \textit{criterio de corte} es una función que utiliza información de la optimización de un modelo durante dicho proceso (e.g. scoring sobre el conjunto de validación, costo actual, cantidad de iteraciones realizadas) y, en base a una regla establecida, determina si debe frenarse o no. Las reglas más comunes son:

\begin{itemize}
	\item \textit{Máximo de iteraciones:} Se detiene la optimización luego de que el número de iteraciones sobre los datos exceda un valor máximo establecido.
	
	\item \textit{Alcanzar un valor mínimo deseado:} Se establece una tolerancia para un determinado valor de información en la optimización (como el scoring o el costo), con lo cual el proceso se frena cuando el valor se alcanza o supera.
	
	\item \textit{Tiempo transcurrido:} Luego de exceder un intervalo de tiempo máximo fijado (en segundos, por lo general), se detiene el proceso de optimización.
	
\end{itemize}

Cada una de estas reglas devuelve Verdadero si el proceso debe frenar o Falso en caso contrario. Dichos resultados booleanos pueden combinarse con operadores lógicos AND y OR para armar reglas más expresivas que configuren la optimización de una forma más específica. Por ejemplo, se puede establecer que el proceso tenga un máximo de 100 iteraciones o bien que frene si se llega a un scoring de 0.9 estableciendo un OR entre las dos primeras reglas explicadas. 

La implementación de ello se encuentra en el módulo \textbf{core.stops}, y se llevó a cabo mediante una adaptación del código provisto por Climin \footnote{Repositorio de código: \url{https://github.com/BRML/climin}} 
% % % %
\iffalse
\makeatletter
\renewcommand\@makefntext[1]{\leftskip=2em\hskip-1em\@makefnmark#1}
\makeatother 
\footnote{ 
		Repositorio de código: \url{https://github.com/BRML/climin} 

		Documentación: \url{http://climin.readthedocs.org}	
} 
\fi
% % % %
con lo cual se puede extender el framework con nuevas reglas y criterios de corte para la optimización de los modelos.

Notar que para implementar este esquema de criterios en Learninspy, se deben especificar dos tipos de configuración: una para la optimización de los modelos réplicas a entrenar en paralelo sobre un batch de datos (llamada ``optimización local''), y otra para la optimización en general del modelo final respecto a un conjunto de validación (denominada ``optimización global''). En la Sección \ref{cap5:sec:experimentos} del capítulo siguiente, un experimento de validación está dedicado a mostrar de forma empírica la relación entre ambos tipos de optimización.


%-------------------------------------------------------------------
\subsection{Esquemas similares}
%-------------------------------------------------------------------
\label{cap4:subsec:esquemas}


En términos de comparación respecto a los esquemas mencionados en la Sección \ref{cap3:subsec:paralel-modelos}, se identifcan las siguientes ventajas del esquema propuesto en este trabajo:

\begin{itemize}
	\item \textbf{Simplicidad}: Gracias a las primitivas que provee Spark, implementar el esquema es sencillo y requiere pocas líneas de código para lograr que la optimización sea concurrente y además escale en recursos. %(adjuntar ejemplo en Python?).
	
	\item \textbf{Convergencia}: Dado que se sincronizan las actualizaciones en cada iteración mediante el mezclado, se mitiga el riesgo de divergencia en la optimización que padecen tanto Downpour SGD como HOGWILD!, convergiendo a una solución comparablemente óptima con la desarrollada por el SGD sin paralelizar.
	
	\item \textbf{Elección del algoritmo de optimización}: Ya que el esquema es independiente del algoritmo utilizado para optimizar un modelo, se pueden implementar diversos tipos de algoritmos iterativos que estén basados en gradiente como los mencionados en la Sección \ref{cap2:subsec:optimizacion}. Los mismos se pueden desarrollar en el módulo \textit{core.optimization} del framework, donde actualmente se proveen dos algoritmos para optimizar redes neuronales.
	
	\item \textbf{Reproducibilidad de resultados}: En H2O, una plataforma que utiliza HOGWILD! para optimizar redes neuronales profundas, se debe ejecutar el entrenamiento con un único hilo de ejecución para obtener resultados replicables debido a las limitaciones del esquema. En Learninspy dicha reproducibilidad se logra independientemente del paralelismo empleado (por lo que se mencionó antes en la Sección \ref{cap4:sec:features}), lo cual se ve como una ventaja muy importante a la hora de experimentar.
	
	\item \textbf{Personalización}: El mezclado de modelos no necesariamente se debe hacer promediando las contribuciones (como sucede en Iterative MapReduce y HOGWILD!), sino que se puede diseñar la función de consenso que decide cómo ponderar las mismas e incorporarla fácilmente en el framework. Por defecto se incluyen las tres funciones explicadas anteriormente.
	
\end{itemize}

De los tres algoritmos tratados en la comparación, se considera que el propuesto en este trabajo se asemeja mayormente al denominado Iterative MapReduce, ya que ambos incoporan la metodología de una tarea MapReduce en cada iteración de la optimización en un modelo. No obstante, en Learninspy se decidió implementar un esquema propio para definir el entrenamiento distribuido de una forma que, al igual que otras características de este framework, sea flexible y entensible respecto a las funcionalidades involucradas, asegurando además la propiedad de escalabilidad buscada.	



\iffalse
Futuros trabajos:

\begin{itemize}
	
	\item Feature: Estaría bueno que se estime el tiempo aprox. de entrenamiento.
	
	\item Feature: Estaría bueno implementar un módulo de scheduler en un futuro, como el de climin http://climin.readthedocs.org/en/latest/schedule.html
\end{itemize}
\fi


% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:
