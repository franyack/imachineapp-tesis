%---------------------------------------------------------------------
%
%                          Capítulo 3
%
%---------------------------------------------------------------------

\chapter{Descripción del sistema}


\begin{FraseCelebre}
	\begin{Frase}
		La manera de empezar es dejar de hablar y empezar a hacer.
	\end{Frase}
	\begin{Fuente}
		Walt Disney
	\end{Fuente}
\end{FraseCelebre}

\begin{resumen}
En este capítulo se describe en primer lugar el diseño elegido para el sistema, explicando cada uno de sus componentes y justificando su composición. Luego, se explican todas las cuestiones técnicas relacionadas al motor de procesamiento, como por ejemplo la manera en que se caracterizan las imágenes, como se mide la similitud entre ellas, como se aplica el algoritmo MCL para obtener los grupos finales, etc. Por último, se presentan las herramientas con las que cuenta el usuario tanto para su experiencia durante la utilización de la aplicación como para administrar el resultado final.
\end{resumen}

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/imachineapp-logo}
		}
		\caption{Ícono de la aplicación desarrollada.}
		\label{fig:imachinapp-logo}
	\end{center}
\end{figure}

El producto desarrollado en este proyecto se identifica como \emph{IMachineApp}, donde la I al inicio de su nombre simboliza las imágenes a procesar, la palabra \textit{Machine} hace referencia a la ``Máquina'' por la cual pasan las mismas para poder obtener el resultado. Por último, la sigla App deja en claro que el producto está destinado a ser desplegado en dispositivos móviles (ver Figura \ref{fig:imachinapp-logo}).





%-------------------------------------------------------------------
\section{Diseño del sistema}
%-------------------------------------------------------------------
\label{cap3:sec:estructura}

En esta tesis se desarrolló un software que se conoce como aplicación móvil. La misma fue diseñada para ser ejecutada en teléfonos inteligentes y \textit{tablets}, y permite al usuario elegir un directorio (o un conjunto de ellos) con imágenes y luego de un procesamiento por el que pasan las mismas, sugerirle una estructura organizacional, basándose en el rango de similitud o contexto que tengan entre ellas. A su vez, cuando finaliza el proceso, tiene la posibilidad de administrar el resultado, según su criterio para obtener así el resultado esperado, o bien deshacer todos los cambios y volver las imágenes hacia su estado original.

La aplicación, fue diseñada para que todo el proceso ocurra en 4 etapas:

\begin{enumerate}
	\item \underline{Elección del/los directorio/s a procesar:} consiste en elegir algún directorio del dispositivo que cuente con imágenes a procesar, como por ejemplo donde quedan alojadas aquellas fotos que fueron tomadas por las cámaras del dispositivo.
	
	\item \underline{Procesamiento de las imágenes a través del motor:} esta etapa cuenta con una serie de pasos que se encarga de extraer las características de las imágenes, procesar las mismas afín de medir su similitud y poder obtener un conjunto de grupos donde en cada uno de ellos estarán las imágenes que más relación tengan entre sí. Esta  etapa constituye el \textit{core} de la aplicación.
	
	\item \underline{Administración del resultado arrojado por el procesamiento:} una vez que el motor terminó con el proceso de las imágenes y encontró una estructura de organización sugerida, en esta etapa el usuario podrá realizar tareas típicas como eliminar, mover o copiar imágenes entre directorios, o bien eliminar un directorio completo, moverlo hacia otra ubicación, etc.
	
	\item \underline{Administración del resultado final:} luego de que la etapa de administracón de las carpetas sugeridas por el proceso ha finalizado, el usuario tendrá la posibilidad de tomar diferentes decisiones respecto al resultado obtenido: descartar todos los cambios y volver las imágenes a su versión original, moverlas hacia una nueva carpeta creada por el programa, o copiarlas y mantenerla en dos lugares diferentes.
\end{enumerate}

\subsection{Metodología de trabajo}

Para el cumplimiento óptimo de todos los objetivos del proyecto, se implementó el ciclo de vida del software siguiendo un modelo iterativo denominado Desarrollo Rápido de Aplicaciones (\textbf{RAD} del inglés, \textit{Rapid Application Development}). Fue creado en 1980 por James Martin, siguiendo el modelo tradicional de desarrollo de software en cascada \cite{beynon1999rapid}. El modelo está compuesto por cuatro etapas, donde en la primera de ellas se realiza la definición de requerimientos, siguiendo por el diseño del sistema, el desarrollo del mismo y por último la etapa donde el producto está listo para ser enviado a producción. Si bien tiene un grado de similitud al modelo en cascada, en las etapas de diseño e implementación se trabaja de forma iterativa e incremental hasta cumplir con los criterios de aceptación dispuestos (ver Figura \ref{fig:rad-model}).

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/rad-model}
		}
		\caption{Modelo RAD (IMAGEN A CAMBIAR)}
		\label{fig:rad-model}
	\end{center}
\end{figure}

Se consideró a RAD el tipo de metodología que mejor se adaptaba al proyecto, en base a las diferentes etapas que lo conformaban y a la manera en que se trabajaría en cada una de ellas. Esta decisión se debe a que el proyecto tuvo una componente fundamental que fue la experiencia de usuario, y como se ha podido observar en publicaciones académicas que comparan diferentes metodologías para llevar adelante proyectos de software, se considera que aquellas de desarrollo lineal (como el modelo en cascada, por ejemplo) no son apropiadas \cite{burns1985selecting}. Esto se debe a que los requerimientos de diseño se cumplieron de forma iterativa e incremental y no estrictamente en una única fase del proyecto, a fin de verdaderamente hacer lo que se solicitaba. En base a los estudios señalados anteriormente, los componentes relacionados a la experiencia de usuario se fueron incorporando y mejorando de forma iterativa gracias a la retroalimentación o \textit{feedback} que se obtuvo por parte de los \textit{stakeholders}. 
 
Una vez definida la ERS (Especificación de los requerimientos del software), el producto se realizó en cuatro versiones de manera incremental, donde en cada una de ellas se entregaron avances, sujetos a lo especificado en el documento predefinido. Al término de cada versión, se obtuvo un prototipo del producto, para luego poder refinar y evaluar qué cosas eran necesarias cambiar, agregar o quitar, con el objetivo de que se cumplan exitosamente los criterios de aceptación de los entregables que conformaban el proyecto. Cada versión estaba compuesta por una parte de diseño, implementación y testeo, y son denominadas por la metodología como \textit{time boxing} \cite{beynon1999rapid}. La finalización de todas las versiones conformó el producto buscado.

A continuación, se explicitan cada uno de los prototipos que conformaron la realización del proyecto, especificando lo que se realizó en cada una de ellos. Cabe aclarar una vez más que el producto cuenta con dos bloques principales a diseñar e implementar. El primero de ellos es la parte encargada de la extracción de características de las imágenes, procesamiento de las mismas y agrupamiento, y el otro bloque está conformado por la aplicación, encargada de realizar la gestión y administración del resultado obtenido luego del procesamiento.

\begin{itemize}
	\item \textbf{Prototipo 1:} Integración inicial de tecnologías y componentes de desarrollo.
	\begin{itemize}
		\item \underline{Sistema Android:} La tarea que debió cumplir fue mostrar al usuario un conjunto de directorios, pero siempre eligiendo uno por defecto. Este directorio ingresaba al motor de procesamiento. Después de que se ejecutaba el motor, debía mostrar por pantalla que el proceso había sido exitoso.
		
		\item \underline{Motor de procesamiento:} debía levantar la biblioteca TensorFlow y leer las imágenes ingresadas por la aplicación, devolviendo una simulación de grupos resultado para el conjunto de imágenes procesadas.
	\end{itemize}
	
	\item \textbf{Prototipo 2:} Agregación de nuevas características al sistema e implementación de la metodología de clustering en el motor.
	\begin{itemize}
		\item \underline{Sistema Android:} se agregó a lo ya realizado la opción de elegir un directorio determinado. Luego del proceso, debía crear directorios en el dispositivo y copiar cada imagen al directorio correspondiente.
		
		\item \underline{Motor de procesamiento:} se aplicó las metodologías definidas para realizar el procesamiento, devolviendo al sistema a qué grupo debía ir cada una de las imágenes.
	\end{itemize}
	
	\item \textbf{Prototipo 3:} Continuación del desarrollo de nuevas funcionalidades y presentación formal del motor de procesamiento.
	\begin{itemize}
		\item \underline{Sistema Android:} debía permitir la utilización de las diferentes tareas de administración una vez que el procesamiento fue realizado. Además, se agregó la opción de guardar el resultado final o eliminarlo y volver todo al estado original.
		
		\item \underline{Motor de procesamiento:} sufrió modificaciones a fin de lograr mejores resultados, además se trabajó en tareas de optimización.
	\end{itemize}


	\item \textbf{Prototipo 4:} Finalización del desarrollo e integración final de todos los componentes.
	\begin{itemize}
		\item \underline{Sistema Android:} debía presentar todas las interfaces de usuario terminadas y listas para utilizar, el ícono que distingue a la aplicación y la aprobación de todas las pruebas de validación.
		
		\item \underline{Motor de procesamiento:} sufrió modificaciones a fin de lograr mejores resultados. Debió superar las pruebas presentadas en el protocolo de experimentación.
	\end{itemize}
\end{itemize}

\subsection{Patrón de programación seleccionado}

Como se dijo en la Sección \ref{cap2:sec:dispositivos-moviles}, la aplicación será desarrollada para el sistema operativo Android, de manera nativa, afín de aprovechar todas las ventajas que las librerías proveen para los desarrolladores. El framework Android, no recomienda ninguna forma específica de diseñar aplicaciones. Eso, de alguna manera, hace a los desarrolladores más poderosos y vulnerables, al mismo tiempo \cite{mvp2017guide}.

Las aplicaciones pasan por muchos ciclos de vida donde se van agregando/extrayendo características, ocasionando estragos si la misma no se diseña adecuadamente con una ``separación de preocupaciones''. No es una buena práctica cargar de lógica a las actividades que se encargan de interactuar con el usuario, sino mas bien deberían ser clases por las cuales simplemente la información que interactúa entre el sistema y el usuario fluye, desligándose de cualquier tipo de lógica de negocio. Es por ello, que se decidió seguir el desarrollo de la aplicación bajo un patrón de diseño Modelo-Vista-Presentador (\textbf{MVP}).

\subsubsection{Modelo-Vista-Presentador}

Este patrón de diseño es un conjunto de pautas que, si se siguen de manera correcta, desacoplan el código para su reutilización y testeabilidad. Básicamente, divide los componentes de la aplicación basados en su función, denominado \textit{separación de preocupaciones}, es decir, MVP divide a la aplicación en tres componentes básicos \cite{mvp2017guide}:

\begin{enumerate}
	\item \textbf{Modelo:} es el responsable de manejar todo lo que corresponda a la lógica de negocio de la aplicación, procesando la información y tomando decisiones.
	
	\item \textbf{Vista:} es la responsable presentar las vistas al usuario e interactuar con el mismo.
	
	\item \textbf{Presentador:} es un puente que conecta al modelo con la vista. Además, actúa como un instructor para la vista.
\end{enumerate}

Definidos cada uno de los componentes, MVP establece algunas reglas básicas para ellos:

\begin{itemize}
	\item La única responsabilidad de una Vista, es dibujar la interfaz de usuario según las instrucciones del Presentador. Es la parte ``tonta'' de la aplicación.
	\item La Vista delega todas las interacciones con el usuario hacia el Presentador.
	\item La Vista \textbf{nunca} se comunica con el Modelo directamente.
	\item El Presentador es el responsable de delegar los requerimientos de la vista hacia el Modelo, y instruir a la Vista con acciones para eventos específicos.
	\item El Modelo es el responsable de realizar toda la lógica de negocio requerida, como por ejemplo obtener los datos desde el servidor, la base de datos o el sistema de archivos.
\end{itemize}

A modo de resumen, cada Vista tendrá una relación 1 a 1 con un Presentador, el cual se encargará de llevar y traer toda la información, indicándole a la Vista que acción y en qué momento se debe ejecutar. Dicha comunicación se establece a través de Interfaces, las cuales servirán como \textit{contratos} para dejar pre-establecidos que medios de comunicación existirán entre ellos. De manera similar y bajo el mismo criterio, el Presentador se comunicará con el Modelo a través de Interfaces, pero sin la necesidad de que la relación entre ellos sea estrictamente 1 a 1, ya que el modelo podría modularizarse tanto como se requiera. De esta manera, el Modelo se desentiende de los eventos y sólo es llamado cuando se lo requiere (ver Figura \ref{fig:mvp-pattern}).

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/mvp-pattern}
		}
		\caption{Patrón de diseño Modelo-Vista-Controlador}
		\label{fig:mvp-pattern}
	\end{center}
\end{figure}

La aplicación fue pensada en dos bloques diferentes. El primero se corresponde a la parte principal, donde se elige el directorio a procesar y se lleva adelante toda la lógica para lograr la sugerencia de organización de las imágenes. El segundo, ya tiene más que ver con las opciones de administración sobre el resultado, con la posibilidad de guardarlo, de mover todas las imágenes hacia un nuevo directorio final o bien descartar todos los cambios. En la Figura \ref{fig:IMachineAppMVP} se puede ver en lineas generales el diseño de la aplicación.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/IMachineAppMVP}
		}
		\caption{Diseño de la aplicación IMachineApp}
		\label{fig:IMachineAppMVP}
	\end{center}
\end{figure}

Como conclusión y aprendizaje, la arquitectura es en lo primero que se debe trabajar para cualquier software. Una arquitectura cuidadosamente diseñada, minimizará muchas repeticiones en el futuro, a la vez que proporcionará la facilidad de escalabilidad. La mayor parte de proyectos se desarrollan hoy en día en equipo, por lo que la legibilidad y la modularidad del código deben considerarse como elementos sumamente importantes de la arquitectura. También, se depende en gran medida de bibliotecas de terceros, cambiando entre alternativas, debido a casos de uso, errores o soporte. 

\section{Motor de procesamiento de las imágenes}

En esta sección se detallará el paso a paso por el que transita el conjunto de imágenes para que se puedan obtener los grupos sugeridos por la aplicación, ordenándolas según el grado de similitud o contexto que tengan entre ellas.

El motor de procesamiento de las imágenes se llama \textbf{CIEngine}. El nombre está compuesto por las letras C, haciendo referencia a la palabra \textit{Clustering} (agrupamiento en inglés), la letra I de \textit{Images}, y por último la palabra \textit{Engine}, que en inglés significa motor. El mismo fue pensado y realizado como un módulo aparte de la aplicación propiamente dicha, y agregado a través de un paquete. Esto tiene como beneficios que sea independiente, por lo que la mantenibilidad y testeo se realizan sin depender exclusivamente de la aplicación, además de que se puede pensar como trabajo a futuro en la adaptación a otros ambientes, como puede ser en un programa escritorio, un servicio alojado en la nube, una aplicación nativa para otros sistemas operativos utilizados en dispositivos móviles, etc.

En la Figura \ref{fig:CIEngineStep-by-step} se puede observar el camino que recorren las imágenes dentro del motor, desde la lectura de todas ellas hasta la conformación de cada uno de los grupos resultantes. Las tareas fueron divididas en etapas, afín de que el trabajo esté estructurado y sea más entendible, localizando en cada una de ellas diferentes técnicas, relacionadas entre sí. A continuación, se describe en detalle cada uno de los pasos.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/CIEngineStep-by-step}
		}
		\caption{Diseño del motor de procesamiento}
		\label{fig:CIEngineStep-by-step}
	\end{center}
\end{figure}

\subsection{Etapa 1: Pre-procesamiento y Extracción de Características}

Como se dijo en el capítulo anterior, se utiliza la biblioteca TensorFlow, específicamente un modelo de red neuronal denominado MobileNet, para realizar la descripción y extracción de características de las imágenes. Se eligió este tipo de modelo ya que se encuentran preparados y optimizados para ser utilizados en dispositivos móviles, logrando buenos resultados en ambientes productivos ya que son hoy por hoy el estado del arte en lo que corresponde a problemas de clasificación de imágenes \cite{introCNN}. El modelo tiene un tamaño total de 16.9MB, ideal para el uso en una aplicación móvil, ya que se busca que la misma no ocupe tanto espacio, destacando la no necesidad de una conexión a Internet, haciendo todo el proceso \textit{on premise}, es decir, explotando los recursos del dispositivo.

La red utilizada en este proyecto fue entrenada con 1001 grupos pertenecientes a la base de datos de imágenes \textit{ImageNet}. Esta base pertenece a un proyecto que fue diseñado para el uso de sistemas relacionados al reconocimiento de objetos, ya que cuenta con más de 14 millones de URL's de imágenes que han sido anotadas a mano por personas para indicar qué son los objetos que se encuentran en cada una de las imágenes. La base de datos cuenta con más de 20.000 categorías, y si bien las anotaciones de las imágenes realizadas por los colaboradores está disponible de manera gratuita, las imágenes reales no son propiedad de \textit{ImageNet} \cite{ImageNet}.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/MobileNet2}
		}
		\caption{Ejemplo de aplicación de una MobileNet}
		\label{fig:MobileNet}
	\end{center}
\end{figure}

La forma de procesar que tiene la red es: a partir de una imagen de entrada, devolver el grado de similitud que tiene con los 1001 grupos con los que cuenta la red entrenada. Esto significa que por cada uno de los grupos, se obtendrá como salida un número entre 0 y 1, indicando el porcentaje de similitud. La suma de los 1001 resultados, será 1 (ver Figura \ref{fig:MobileNet}.

A su vez, estos grupos están organizados acordes a la jerarquía de etiquetas o \textit{tags} que tiene \textit{WordNet}, una gran base de datos léxica para el idioma inglés, compuesta de sustantivos, verbos, adjetivos y adverbios que se agrupan en conjuntos, donde cada uno de ellos expresan un concepto distinto. Para el uso de \textit{ImageNet}, sólo se utilizaron \textit{tags} sustantivos, y los mismos están jerarquizados partiendo desde uno madre o genérico, bajando hacia aquellos que detallan en concreto el/los objetos que se encuentran en la imagen (ver Figura \ref{fig:WordNetExample}). Un punto importante es que esta base también es libre y gratuita, enfocada principalmente en el procesamiento automático del lenguaje natural y en aplicaciones de inteligencia artificial \cite{WordNet}.

\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/WordNetExample}
		}
		\caption{Ejemplo del uso de etiquetas - WordNet}
		\label{fig:WordNetExample}
	\end{center}
\end{figure}

Dichas anotaciones, tienen relación con una base de datos de etiquetas o \textit{tags} que fue organizada acorde a la jerarquía de palabras que se encuentran en la base de datos léxica WordNet. Cada una de las palabras que se encuentran allí están internacionalizadas por medio de relaciones conceptuales-semánticas y léxicas. Como sucede con ImageNet, esta base de datos también es libre y pública por lo que se encuentra disponible para su descarga.

\subsubsection{Pre-procesamiento de las imágenes}


%Se cargan las imagenes con menos resolucion (ocupando menos espacio en memoria)
%Se escala la imagen para lograr la misma de 224 x 224
%Se manda la imagen a extraer las caracteristicas (ver si es necesario explicar BitMap -> ByteBuffer)

Para que las imágenes puedan ser interpretadas por la red neuronal, deben pasar por un pre-procesamiento. Debido a una restricción que se tuvo cuando entrenaron los distintos modelos que componen el grupo MobileNet, la red que obtiene mejores resultados en términos de clasificación fue entrenada con imágenes cuyo tamaño fue de 224x224 píxeles, por lo tanto, para poder utilizarla, las imágenes de entrada al motor de procesamiento deben tener este tamaño también. Para lograr ello, las imágenes son cargadas en memoria a través de un objeto que provee Android denominado Bitmap, el cual permite realizar distintas operaciones sobre ellas \cite{Bitmap}. Con el objetivo de reducir los recursos computacionales utilizados, las mismas son cargadas en memoria bajando su resolución a un tamaño en el cual su ancho y/o alto es de 224 píxeles, adaptando la otra medida afín de que la imágen siga siendo proporcional. Una vez realizado esto, luego si es estrictamente re-escalada al tamaño solicitado por la red. Entonces, una vez que ya se encuentra en memoria la imagen con una resolución de 224x224 píxeles lista para ser procesada por la CNN, se convierte en un objeto java denominado ByteBuffer \cite{ByteBuffer}, el cual es necesario en este formato ya que así lo solicita la red neuronal (ver Figura \ref{fig:preProcessing}).     
 
 
\begin{figure}[t]
	\begin{center}
		\scalebox{0.7}{
			\includegraphics[width=0.8\textwidth]%
			{Imagenes/Bitmap/preProcessing}
		}
		\caption{Pre-procesamiento de las imágenes}
		\label{fig:preProcessing}
	\end{center}
\end{figure}

\subsubsection{Extracción de características}
%Se obtiene un resultado con los 5 (si existen) mayores tags resultado cuya correspondencia es > 0.05f
%Se obtiene el resulado de los embeddings
Una vez que la imagen pasa por la red neuronal, se obtienen como predicciones dos resultados:

\begin{enumerate}
	\item \textbf{Resultado 1: etiquetas y probabilidades}
	
	La primer salida que se obtiene de la red es una lista de las 5 mayores etiquetas o \textit{tags} (en caso de que existan) cuya probabilidad de similitud sea mayor o igual a un umbral establecido. Una vez que se obtienen dichos \textit{tags}, se busca por cada uno de ellas las 4 etiquetas ``padres'' (utilizando la jerarquía definida por \textit{WordNet}) asignándoles la misma probabilidad que la etiqueta ``hija'' o inicial.
	
	\item \textbf{Resultado 2: caracterización de la imagen}
	
	La segunda salida que se obtiene de la red es un indicador sobre cómo se activó cada neurona en la última capa antes de que se realice la clasificación. Esta última capa tiene como nombre \textit{embeddings} y es utilizada en el proceso como una caracterización de la imagen por parte de la red. Los \textit{embeddings} son importantes para el aprendizaje maquinal, ya que se convierten datos a una representación de características donde ciertas propiedades pueden ser representadas por nociones de distancia. Un modelo entrenado para clasificar imágenes puede permitir convertir una imagen en un vector de números, de manera que otra imagen similar tenga una distancia (por ejemplo, euclidiana) pequeña \cite{Embeddings}.  
\end{enumerate}

Como conclusión, se obtiene por cada una de las imágenes a procesar un conjunto de etiquetas y probabilidades de correspondencia, y un vector con valores numéricos que indica como la red neuronal caracterizó a la imagen. A continuación, se explicará la etapa 2, en donde se realiza la medición de similitudes entre las imágenes.

\subsection{Etapa 2: Medición de similitud entre las imágenes}
\label{cap3:subsec:affinity}
Una vez que fue recolectada toda la información que se describió durante la sección anterior, se procede a medir el grado de similitud que existe entre cada par de imágenes, basándose en las etiquetas y probabilidades que se obtuvo de cada una de ellas, como así también del vector de características provisto por la capa de \textit{embeddings}. Al final de esta etapa, se obtendrá una matriz que será utilizada por el algoritmo de \textit{clustering}. Esta matriz, está conformada por dos matrices que reciben el nombre de \underline{matrices de afinidad}, las cuales se describen a continuación:

\begin{enumerate}
	\item \textbf{Matriz de afinidad gramatical:} 
	
	Esta matriz indica la afinidad gramatical que existe entre cada par de imágenes teniendo en cuenta los \textit{tags} obtenidos en la etapa de extracción de características, de allí la elección de su nombre. Por lo tanto, se obtendrá una matriz de tamaño NxN, siendo N el número de imágenes a procesar, y la posición \textit{i,j} dentro de la matriz el grado de similitud gramatical que existe entre la imagen i y la imagen j.
	
	Para poder medir dicha similitud, lo que se hace en una primera instancia es obtener una especie de \textit{diccionario} con todos los \textit{tags} existentes, provistos por todas las imágenes incluidas en el proceso. Luego, por cada una de las imágenes, se confecciona un mapa que tiene como clave todos los \textit{tags} provistos por el diccionario y como valor la probabilidad de similitud obtenida en la sección anterior, en caso de que exista ocurrencia, para aquellos \textit{tags} que no se corresponden con la imagen directamente se les asigna como valor un 0. De esta manera, se logra un vector normalizado por cada una de las imágenes, por lo que resta medir la similitud que existe entre cada una de ellas. Para obtener dicha medida, se utiliza el \textbf{coeficiente de correlación de Pearson}, un índice estadístico que puede utilizarse para medir el grado de correlación que tienen dos vectores \cite{benesty2009pearson}. El valor puede variar entre [-1,1], siendo 1 una correlación perfecta. El criterio que se tomó en este proyecto es quedarse solamente con aquellas correlaciones que superan un umbral de 0.65, y en cualquier otro caso directamente se toma como inexistente una correlación entre ambas imágenes. De esta manera, se llega a la obtención de la matriz de afinidad gramatical entre las imágenes.
	
	\item \textbf{Matriz de afinidad según embeddings:} 
	
	De manera similar a como se obtuvo la matriz de afinidad gramatical, se procesan cada uno de los vectores que se obtuvieron por caracterizar a la matriz gracias a la capa de \textit{embeddings}. Por cada imagen existe un vector de 1024 números(cantidad de neuronas que conforman la capa), el cual es normalizado y medido de a pares a través del coeficiente de Pearson, con el objetivo de obtener una nueva matriz de NxN, similar a la obtenida anteriormente. 
\end{enumerate}

Una vez que se obtienen estas dos matrices cuadradas del mismo tamaño que la cantidad de imágenes que se están procesando, lo que se hace es una ponderación 50/50 (es decir, se suman los valores i,j de cada una y se divide por 2), logrando una única matriz de afinidad de las imágenes. Finalizada esta etapa, el proceso se encuentra en condiciones de pasar a la etapa 3, en el cual se utiliza el algoritmo de \textit{clustering}.

\subsection{Etapa 3: Ejecución del algorítmo de clustering}

Tal como se dijo en el capítulo anterior, el algoritmo elegido para realizar el agrupamiento de las imágenes según el grado de similitud o contexto que ellas tengan es el de Markov (MCL). Perteneciendo a la familia de algoritmos basados en grafos, MCL encuentra grupos de items de manera natual, sin la necesidad de indicarle a priori la cantidad que se esperan como resultado.

El algoritmo depende de 4 parámetros a configurar, los cuales se describen a continuación:

\begin{itemize}
	\item \textbf{Potencia de expansión:} valor que varía entre 2 y 3, el cual indica cuánto deben explorarse los grafos obtenidos a partir de la matriz de afinidad.
	
	\item \textbf{Potencia de inflación:} valor que varía entre 2 y 4. A medida que se aumenta este valor, crece la cantidad de grupos que se obtendrán.
	
	\item \textbf{Umbral de corte:} diferencia máxima que debe existir entre la matriz de agrupamiento actual contra la matriz de agrupamiento en la iteración anterior, para que el algoritmo converja y devuelva el resultado.
	
	\item \textbf{Cantidad máxima de iteraciones:} variable definida con el objetivo de	poner un freno a la cantidad de iteraciones del algoritmo en caso de que el umbral de corte nunca llegue a ser menor o igual al establecido.
\end{itemize}

Si bien se recomienda que los mismos sean configurados entre ciertos valores, la realidad es que se terminan definiendo con el correr de las pruebas, es decir de una manera estocástica, ya que se ajustan según la problemática a tratar y el protocolo de experimentación que se siguió durante el desarrollo.  

Pasando ahora a describir el algoritmo, éste realiza dos operaciones algebraicas simples en matrices, y no requiere ningún tipo de instrucción para unir, ensamblar o dividir los grupos. A continuación, se explica paso a paso: 

\begin{enumerate}
	\item \underline{Paso 1:} Una vez que recibe la matriz, llena la diagonal con valores 1. Este ejercicio se denomina \textit{self-loop} o ``auto-bucle'' y asegura que el nodo se agrupe consigo mismo, ya que siempre hay una ruta mas corta desde un nodo hacia sí mismo. Se supone que el algoritmo funciona sin él, pero se obtendrían efectos poco comunes para nodos que no se encuentran en vecindarios con un alto coeficiente de agrupación. Luego, una vez realizado el auto-bucle, se normaliza la matriz.
	
	\item \underline{Paso 2:} mientras que el algoritmo \underline{no converja} o llegue al \underline{máximo de iteraciones de corte}, se aplican las dos operaciones mencionadas arriba:
	\begin{enumerate}
		\item \textbf{Etapa de expansión:} se aplica una potencia a la matriz elevándola por el coeficiente de expansión \textit{e}. 
		\item \textbf{Etapa de inflación:} A la matriz resultado de la etapa anterior, se le elevan todos los valores (i,j) por el coeficiente de inflación \textit{p} definido. Esta operación es conocida como una potencia de Hadamard. Una vez finalizado esto, se vuelve a aplicar una normalización.
		\item Por último, se compara la matriz actual contra la matriz en la iteración anterior, y se repite el proceso hasta que haya \textit{convergido} o bien supere la \textit{cantidad máxima de iteraciones.} 
	\end{enumerate} 
	
	\item \underline{Paso 3:} se retorna la matriz final, la cual indica que aquellas posiciones (i,j) distintas de 0 que compartan la fila i, pertenecen al mismo \textit{cluster} o grupo.
\end{enumerate}

\subsection{Etapa 4: Post-proceso}

Una vez que se obtuvieron los grupos conformados por aquellas imágenes que según el motor de procesamiento tienen un grado de similitud o parecido entre ellas, se trata aquel escenario donde el resultado final de un grupo (o grupos) esté conformado por una sola imagen.

El procedimiento que se sigue para estos casos es el de detectar aquellas imágenes que quedaron agrupadas solas por el motor, y buscar por cada una de ellas en la matriz de afinidad (detalla en la sección \ref{cap3:subsec:affinity}) la imagen que corresponde a la mayor afinidad que existe para ella, y directamente agregarla al grupo donde está contenida la segunda. Es decir, si existe una única imagen agrupada X, se busca en la matriz de afinidad la imágen Y que tenga la mayor afinidad que X tiene con otra imagen. Luego, se encuentra el \textit{cluster} donde está contenido Y y se suma X a ese grupo. 

En caso de no existir dentro de la matriz ninguna imagen con la que tenga afinidad, se pondrá dentro de una carpeta destinada a aquellas imágenes que no tengan ningún parecido, y así evitar agrupaciones con una sola imagen.
 

